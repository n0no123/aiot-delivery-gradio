{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hi! You can find a live demo of the app [here](https://huggingface.co/spaces/Epitech/hand-sign-detection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information\n",
    "As of today, there are about 70 million deaf people in the world and they collectively use more than 300 different sign languages.\n",
    "This software will allow people using sign language to communicate with people who do not know it.\n",
    "\n",
    "### How does it work?\n",
    "Using a camera (for example the one on your phone) film a person, and let our IA transcript the sign on your screen.\n",
    "*For now, only the words: **eat**, **drink**, **watch** and **ok** are recognizable by our software.* \n",
    "\n",
    "### Who'll use it?\n",
    "Any persons who wich to communicate with someone speaking sign language.\n",
    "This could happen in lot of scenarios like when an hearing impaired wants to speak with:\n",
    "- a merchant\n",
    "- a doctor\n",
    "- a police \n",
    "\n",
    "### What technologies are behind:\n",
    "For training:\n",
    "- Mediapipe\n",
    "- OpenCV\n",
    "- Teachable\n",
    "\n",
    "For the IA:\n",
    "- Keras\n",
    "- Tensorflow\n",
    "\n",
    "For the code base:\n",
    "- Python\n",
    "\n",
    "For the app:\n",
    "- Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, please clone the following repository to have access to the necessary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone --branch notebook https://github.com/n0no123/hand-sign-detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r '/content/hand-sign-detection/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gradio as gr\n",
    "import math\n",
    "import numpy as np\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "from cvzone.HandTrackingModule import HandDetector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Set our global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgSize = 96\n",
    "classifier = Classifier('/content/hand-sign-detection/keras_model.h5', '/content/hand-sign-detection/labels.txt')\n",
    "detector = HandDetector(maxHands=1)\n",
    "labels = ['Look', 'Drink', 'Eat', 'Ok']\n",
    "offset = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Set the frame analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(image):\n",
    "    hands, frame = detector.findHands(image)\n",
    "    try:\n",
    "        if hands:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            croppedHand = np.ones((bgSize, bgSize, 3), np.uint8) * 12\n",
    "            imgCrop = frame[y - offset:y + h +\n",
    "                            offset, x - offset:x + w + offset]\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                constant = bgSize / h\n",
    "                wComputed = math.floor(constant * w)\n",
    "                bgResize = cv2.resize(imgCrop, (wComputed, bgSize))\n",
    "                bgResizeShape = bgResize.shape\n",
    "                wGap = math.floor((bgSize-wComputed)/2)\n",
    "                croppedHand[:bgResizeShape[0],\n",
    "                            wGap:wGap + wComputed] = bgResize\n",
    "            else:\n",
    "                constant = bgSize / w\n",
    "                hComputed = math.floor(constant * h)\n",
    "                bgResize = cv2.resize(imgCrop, (bgSize, hComputed))\n",
    "                bgResizeShape = bgResize.shape\n",
    "                hGap = math.floor((bgSize - hComputed) / 2)\n",
    "                croppedHand[hGap: hComputed + hGap, :] = bgResize\n",
    "            _, index = classifier.getPrediction(croppedHand, draw=False)\n",
    "            return labels[index]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return 'No sign detected'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Create the Gradio interface. *To have a better experience, please use the live [demo]((https://huggingface.co/spaces/Epitech/hand-sign-detection).)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.interface.Interface(fn=segment, live=True, inputs=gr.Image(source='webcam', streaming=True), outputs='text').launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
